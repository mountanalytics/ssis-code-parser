{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Truncate table [dbo].[Error_lines]' contains unsupported syntax. Falling back to parsing as a 'Command'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Order_write']\n",
      "['OrderID', 'Dateloop', 'Cat_ID']\n",
      "['User::Sup_id', CurrentTimestamp(), 'User::Cat_id']\n",
      "\n",
      "['SSIS_Logger']\n",
      "['ExecutionStartTime', 'Remark', 'Affected_rows']\n",
      "[CurrentTimestamp(), 'Deleted rows', '0']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pypyodbc as odbc\n",
    "import configparser\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict \n",
    "import pandas as pd\n",
    "import pypyodbc as odbc\n",
    "import configparser\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import sqlglot\n",
    "from sqlglot import parse_one, exp\n",
    "from sqlglot.dialects.tsql import TSQL\n",
    "#from modules.sql_parser.parse_lineages import *\n",
    "#from modules.sql_parser.parse_nodes import *\n",
    "\n",
    "\n",
    "def find_table_w_spaces(tree: sqlglot.expressions):\n",
    "    \"\"\"\n",
    "    Find all table names which have an empty space in them and storing them without the \" \" for later use, as sqlglot cannot parse them otherwise.\n",
    "    \"\"\"\n",
    "    table_names = list(tree.find_all(exp.Table))\n",
    "    space_table = []\n",
    "    for element in table_names:\n",
    "        if \" \" in element.name:\n",
    "            space_table.append((element.name.replace(\" \",\"\"),element.name))\n",
    "    return space_table\n",
    "\n",
    "\n",
    "def extract_target_columns(tree: sqlglot.expressions.Select):\n",
    "    \"\"\"\n",
    "    From the query in input, get all the columns from the select statement\n",
    "    \"\"\"\n",
    "    # extract target columns\n",
    "    select_statement_big = tree.find_all(exp.Select) # find all select statements\n",
    "\n",
    "    select_statement = []\n",
    "    for select in list(select_statement_big): # for every select statements, extract the columns\n",
    "        select_statement += select.expressions \n",
    "\n",
    "    target_columns =[]\n",
    "    for select in select_statement: # for every select statement, find all the target columns and add them to list\n",
    "        columns = list(select.find_all(exp.Column))\n",
    "        target_columns.append([i for i in columns])\n",
    "\n",
    "    return select_statement, target_columns\n",
    "\n",
    "\n",
    "# replace columns aliases\n",
    "def transformer_functions(node):\n",
    "    \"\"\"\n",
    "    Replaces column objects within the functions with simple column names\n",
    "    \"\"\"\n",
    "    if isinstance(node, exp.Column):\n",
    "        return parse_one(node.name)\n",
    "    return node\n",
    "\n",
    "\n",
    "def extract_transformation(tree: sqlglot.expressions.Select):\n",
    "    \"\"\"\n",
    "    Function to extract possible transformation from columns\n",
    "    \"\"\"\n",
    "    # add possible transformation to columns\n",
    "    transformations = []\n",
    "\n",
    "    for col in tree:\n",
    "        if list(col.find_all(exp.Alias)) == []: # if there are no functions\n",
    "            transformations.append(\"\")\n",
    "        else: # else add the function\n",
    "            transformations.append(col.sql(dialect = \"tsql\"))\n",
    "\n",
    "    return transformations\n",
    "\n",
    "\n",
    "def split_at_last_as(input_string: str):  \n",
    "    \"\"\"\n",
    "    Function to split transformation string at last \" AS \", as everything after the last \" AS \" is the alias, not the transformation\n",
    "    \"\"\"\n",
    "    split_point = input_string.rfind(' AS ')\n",
    "    if split_point == -1:\n",
    "        return input_string, ''\n",
    "    return input_string[:split_point], input_string[split_point + 4:]\n",
    "\n",
    "\n",
    "# replace columns aliases\n",
    "def transformer_functions(node):\n",
    "    \"\"\"\n",
    "    Replaces column objects within the functions with simple column names\n",
    "    \"\"\"\n",
    "    if isinstance(node, exp.Column):\n",
    "        return parse_one(node.name)\n",
    "    return node\n",
    "\n",
    "\n",
    "def replace_variables(query:str, variables: list):\n",
    "\n",
    "    \n",
    "    # Convert the string to a list so that we can modify it\n",
    "    result = list(query)\n",
    "    \n",
    "    # Keep track of the replacement index\n",
    "    replace_index = 0\n",
    "    \n",
    "    # Loop through the characters in the string\n",
    "    for i, char in enumerate(result):\n",
    "        if char == '?':\n",
    "            # Replace '?' with the current element in replacements\n",
    "            if replace_index < len(variables):\n",
    "                result[i] = \"'\" + str(variables[replace_index]) +\"'\"\n",
    "                replace_index += 1\n",
    "    \n",
    "    # Convert list back to a string\n",
    "    return ''.join(result)\n",
    "\n",
    "def parse_query(query: str):\n",
    "    \"\"\"\n",
    "    Function to convert query string to a sqlglot parsed tree\n",
    "    \"\"\"\n",
    "    ast = parse_one(query, read=\"tsql\")\n",
    "    trial1 = repr(ast)\n",
    "    return ast\n",
    "\n",
    "def find_select(ast):\n",
    "    selects = list(ast.find_all(exp.Select))   \n",
    "    return selects\n",
    "\n",
    "# parse table name + table alias\n",
    "def parse_tables(table, table_alias_list, subquery=True):    \n",
    "    \"\"\"\n",
    "    Function to parse all table information available (db, catalog...)\n",
    "    \"\"\" \n",
    "\n",
    "    if subquery == False:\n",
    "        table_alias =  table.alias.strip()\n",
    "        table_name = table.name.strip()\n",
    "        table_db = table.db.strip()\n",
    "        table_catalog = table.catalog.strip()\n",
    "\n",
    "    else:\n",
    "        table_alias = table.alias.strip()\n",
    "        source = table.this.args[\"from\"].strip()\n",
    "        table_name= source.this.name.strip()\n",
    "        table_catalog =  source.this.catalog.strip()\n",
    "        table_db = source.this.db.strip()\n",
    "        \n",
    " \n",
    "    if \" \" in table_name:\n",
    "        table_name = table_name.replace(\" \", \"\")\n",
    "        \n",
    "\n",
    "    if table_catalog != \"\" and table_db != \"\":\n",
    "        result = (table_catalog+\".\"+ table_db+\".\"+table_name, table_alias)\n",
    "\n",
    "    elif table_db == \"\" and table_catalog == \"\":\n",
    "\n",
    "        result = (table_name, table_alias)\n",
    "\n",
    "    elif table_catalog == \"\": \n",
    "        result = (table_db+\".\"+table_name, table_alias)\n",
    "\n",
    "    elif table_db == \"\":\n",
    "        result = (table_catalog+\".\"+table_name, table_alias)\n",
    "        \n",
    "\n",
    "    table_alias_list.append(result)\n",
    "    return result\n",
    "\n",
    "def get_tables(ast: sqlglot.expressions.Select):\n",
    "    \"\"\"\n",
    "    Function to extract the table names and their aliases, used to reconstruct a tuple with structure (database+schema+name, alias )\n",
    "    \"\"\"\n",
    "    # find all tables\n",
    "    table_alias = list(ast.find_all(exp.Table))\n",
    "    alias_table = []\n",
    "\n",
    "    # extract information from each table\n",
    "    for table in table_alias:\n",
    "        parse_tables(table, alias_table, False)\n",
    "\n",
    "    return alias_table\n",
    "\n",
    "\n",
    "def replace_aliases(query:str):\n",
    "    # replace aliases\n",
    "    ast = parse_query(query)\n",
    "\n",
    "    alias_table = get_tables(ast)\n",
    "    \n",
    "    def transformer_table(node):\n",
    "        for element in alias_table:\n",
    "            if isinstance(node, exp.Column) and node.table == element[1]:\n",
    "                return parse_one(element[0] + \".\" + node.name)\n",
    "        return node\n",
    "\n",
    "    transformed_tree = ast.transform(transformer_table)\n",
    "\n",
    "    return transformed_tree\n",
    "\n",
    "\n",
    "def get_statements(transformed_tree):\n",
    "    \"\"\"\n",
    "    Function to extract from expression, join expression and where expression from query\n",
    "    \"\"\"\n",
    "\n",
    "    source_tables = []\n",
    "    # from expression\n",
    "    from_exp = list(transformed_tree.find_all(exp.From))\n",
    "    from_table =str(from_exp[0].this).split(' AS')[0] # table\n",
    "    source_tables.append(from_table)\n",
    "\n",
    "    # join expression\n",
    "    join_exp = list(transformed_tree.find_all(exp.Join))\n",
    "    if join_exp != []:\n",
    "        join_table = str(join_exp[0].this).split(' AS')[0] # table\n",
    "        source_tables.append(join_table)\n",
    "    else:\n",
    "        join_exp = None\n",
    "\n",
    "    # where expression\n",
    "    where_exp = list(transformed_tree.find_all(exp.Where))\n",
    "    if where_exp != []:\n",
    "        where_exp = str(where_exp[0].this).split(' AS')[0]# table\n",
    "    else:\n",
    "        where_exp = None\n",
    "\n",
    "\n",
    "    return source_tables, where_exp\n",
    "\n",
    "\n",
    "def on_statement(select_statement: sqlglot.expressions.Select):\n",
    "    \"\"\"\n",
    "    Function to extract the on condition from the join statements, (on column = column)\n",
    "    \"\"\"\n",
    "\n",
    "    # from expression\n",
    "    joins = list(select_statement.find_all(exp.Join))\n",
    "    on_conditions = []\n",
    "    for join in joins:\n",
    "        try:\n",
    "        \n",
    "            on_conditions.append(f\"{list(join.find_all(exp.EQ))[0].this.table}.{list(join.find_all(exp.EQ))[0].this.this} = {list(join.find_all(exp.EQ))[0].expression.table}.{list(join.find_all(exp.EQ))[0].expression.this}\")\n",
    "        except:\n",
    "            return []\n",
    "    if joins != []:\n",
    "        return on_conditions\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "def remove_duplicate_dicts(dict_list):\n",
    "    # Convert list of dictionaries to a list of frozensets (which are hashable)\n",
    "    seen = set()\n",
    "    unique_dicts = []\n",
    "    \n",
    "    for d in dict_list:\n",
    "        # Convert dictionary to a tuple of sorted key-value pairs to make it hashable\n",
    "        t = tuple(sorted(d.items()))\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            unique_dicts.append(d)\n",
    "    \n",
    "    return unique_dicts\n",
    "    \n",
    "\n",
    "def extract_source_target_transformation(target_columns :list, lineages: list, space_table:list, source_node_name:str, target_node_name:str):\n",
    "    \"\"\"\n",
    "    Function that returns a list of dictionaries, in which each dictionary contains the list of source columns, the target column and the possible transformation\n",
    "    \"\"\"\n",
    "    for target_column in target_columns:\n",
    "        source_columns = []\n",
    "\n",
    "        for source_column in target_column[0]:\n",
    "\n",
    "            #parse the table and column info\n",
    "            table = source_column.table\n",
    "            catalog = source_column.catalog\n",
    "            db = source_column.db\n",
    "            column = source_column.name\n",
    "\n",
    "            for w in space_table:\n",
    "                if table == w[0]:\n",
    "                    table = w[1]\n",
    "\n",
    "            if catalog !=\"\" and db !=\"\":\n",
    "                source_column_complete = catalog + \".\" +  db +\".\" + table +\".\" +column\n",
    "\n",
    "            elif catalog == \"\" and db == \"\":\n",
    "                source_column_complete = table +\".\" +column\n",
    "            elif catalog == \"\":    \n",
    "                source_column_complete = db + \".\" + table + \".\"+column\n",
    "            elif db == \"\":\n",
    "                source_column_complete = catalog + \".\" + table +\".\" +column\n",
    "\n",
    "            source_columns.append(source_column_complete)\n",
    "                \n",
    "        if source_columns != []:\n",
    "            if 'AS' in target_column[1]: # if there is an alias, append formula and alias\n",
    "                for col in source_columns:\n",
    "                    if split_at_last_as(target_column[1])[0].strip() not in col:\n",
    "\n",
    "                        lineages.append({'SOURCE_COLUMNS':source_columns, 'TARGET_COLUMN':f\"{target_node_name}.{split_at_last_as(target_column[1])[1].strip()}\", 'TRANSFORMATION':split_at_last_as(target_column[1])[0].strip()})\n",
    "                    else:\n",
    "                        lineages.append({'SOURCE_COLUMNS':source_columns, 'TARGET_COLUMN':f\"{target_node_name}.{split_at_last_as(target_column[1])[1].strip()}\", 'TRANSFORMATION': \"\"})\n",
    "            else:\n",
    "\n",
    "                lineages.append({'SOURCE_COLUMNS':f'{source_node_name}.{source_columns[0].split(\".\")[-1]}', 'TARGET_COLUMN':f'{target_node_name}.{source_columns[0].split(\".\")[-1]}', 'TRANSFORMATION': target_column[1]})\n",
    "    return remove_duplicate_dicts(lineages)\n",
    "\n",
    "def flatten_if_nested(lst):\n",
    "    if len(lst) == 1 and isinstance(lst[0], list):\n",
    "        return lst[0]  # Flatten the list\n",
    "    return lst  # Return the original if not a nested list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def executesql_parser(control_flow, nodes, lineages, variable_tables, node_name):\n",
    "    \n",
    "    try: # try if it is part of for each loop\n",
    "        for i in control_flow[node_name]['SQL']:\n",
    "            variables = flatten_if_nested(control_flow[node_name]['SQL'][i]['Variables'])#[0]     \n",
    "    except:\n",
    "        try:\n",
    "            variables = flatten_if_nested(control_flow[node_name]['Variables'])\n",
    "        except:\n",
    "            pass\n",
    "        pass\n",
    "\n",
    " \n",
    "    try: # try if it is part of for each loop\n",
    "        for i in control_flow[node_name]['SQL']:\n",
    "            sql_statement = control_flow[node_name]['SQL'][i]['SQL_state'] \n",
    "    except:\n",
    "        sql_statement = control_flow[node_name]['SQL_state'] \n",
    "\n",
    "    #print(sql_statement)\n",
    "\n",
    "\n",
    "    # if the sql statement contains a variable, change the ? with the variable name\n",
    "    if '?' in sql_statement: \n",
    "        sql_statement = replace_variables(sql_statement, variables)\n",
    "    \n",
    "    # get sqlglot tree from query\n",
    "    tree = parse_query(sql_statement)\n",
    "\n",
    "    # only parse if there is a select statement\n",
    "    if 'select' in sql_statement.lower():\n",
    "\n",
    "        # find main select statement\n",
    "        select = list(tree.find_all(exp.Select))[0]\n",
    "\n",
    "        # EXTRACT NODES\n",
    "        # parse source tables and where condition\n",
    "        source_tables, where_exp = get_statements(select) \n",
    "        #print(source_tables)\n",
    "        \n",
    "        # parse on condition\n",
    "        on_condition = on_statement(select)\n",
    "\n",
    "        # parse destination table\n",
    "        insert_tables = [table for table in select.find_all(exp.Insert)]\n",
    "        insert_tables += [table.this.this.this for table in select.find_all(exp.Into)]\n",
    "        try:\n",
    "            insert_tables.append(control_flow[node_name]['Result_variable'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # add source tables to nodes\n",
    "        for table in source_tables:              \n",
    "            nodes.append({'NAME_NODE': table.replace(\".\", \"/\"),'LABEL_NODE': table.replace(\".\", \"/\"), 'FILTER': None, 'FUNCTION': 'DataSources', 'JOIN_ARG': None, 'COLOR': \"gold\"})\n",
    "\n",
    "        # add variables to nodes\n",
    "        for variable in variables:\n",
    "            nodes.append({'NAME_NODE': variable,'LABEL_NODE': variable, 'FILTER': None, 'FUNCTION': 'Variable', 'JOIN_ARG': None, 'COLOR': \"green\"})\n",
    "                \n",
    "        # add query node\n",
    "        nodes.append({'NAME_NODE': node_name,'LABEL_NODE': node_name, 'FILTER': where_exp, 'FUNCTION': 'Query', 'JOIN_ARG': on_condition, 'COLOR': 'black'})\n",
    "                \n",
    "        # add destination table to nodes\n",
    "        for table in insert_tables:     \n",
    "            if '::' in table: # if result table is variable\n",
    "                nodes.append({'NAME_NODE': table,'LABEL_NODE': table, 'FILTER': None, 'FUNCTION': 'Variable', 'JOIN_ARG': None, 'COLOR': \"green\"})\n",
    "            else:\n",
    "                nodes.append({'NAME_NODE': table,'LABEL_NODE': table, 'FILTER': None, 'FUNCTION': 'DataDestinations', 'JOIN_ARG': None, 'COLOR': \"gold\"})\n",
    "\n",
    "        # EXTRACT LINEAGES\n",
    "        target_node = insert_tables[0].replace(\".\", \"/\")\n",
    "    \n",
    "        source_node = list(source_tables)[0].replace(\".\", \"/\") # CHANGE THIS IN CASE THERE ARE MORE SOURCE TABLES!!!\n",
    "        query = node_name\n",
    "\n",
    "        space_table = find_table_w_spaces(select) # clean tables\n",
    "        space_table = list(set(space_table)) # a list of tuples with table names paired (space removed original - original ) Eg. (OrderDetails, Order Details)\n",
    "\n",
    "        target_columns = []\n",
    "        select_statement, target_columns = extract_target_columns(tree) # extract target columns\n",
    "        \n",
    "        for table in insert_tables:     \n",
    "            if '::' in table: # if result table is variable\n",
    "                variable_tables[table] = [(col[0].this.this, i) for i, col in enumerate(target_columns)]\n",
    "\n",
    "        replaced_trees = [x.transform(transformer_functions) for x in select_statement] # replace columns aliases\n",
    "\n",
    "        # add possible transformation to columns\n",
    "        transformations = extract_transformation(replaced_trees)\n",
    "        target_columns = list(zip(target_columns, transformations)) \n",
    "\n",
    "        lineages += extract_source_target_transformation(target_columns, lineages, space_table, source_node, query) # append lineages of node to lineages list\n",
    "        lineages += extract_source_target_transformation(target_columns, lineages, space_table, query, target_node) # append lineages of node to lineage list                \n",
    "        \n",
    "        for variable in variables: # append variables to lineages\n",
    "            lineages.append({'SOURCE_COLUMNS':f'{variable}.{variable}', 'TARGET_COLUMN':f\"{query}.{variable}\", 'TRANSFORMATION':\"\"})\n",
    "\n",
    "    elif \"insert into\" in sql_statement.lower():\n",
    "\n",
    "        # find main select statement\n",
    "        insert = list(tree.find_all(exp.Insert))[0]\n",
    "\n",
    "        dest_table = [i.this.this for i in insert.find_all(exp.Table)]\n",
    "        print(dest_table)\n",
    "\n",
    "        schema = [i for i in insert.find_all(exp.Schema)][0]\n",
    "\n",
    "        columns = []\n",
    "\n",
    "\n",
    "        for i in tree.args[\"this\"].args[\"expressions\"]:\n",
    "                            \n",
    "            try:\n",
    "                columns.append(i.args[\"this\"])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        print(columns)\n",
    "\n",
    "        transformations = []\n",
    "\n",
    "        for i in tree.args[\"expression\"].args[\"expressions\"][0].args[\"expressions\"]:\n",
    "            try:\n",
    "                transformations.append(i.args[\"this\"])\n",
    "            except:\n",
    "                transformations.append(i)\n",
    "        print(transformations)\n",
    "\n",
    "        print()        \n",
    "\n",
    "        nodes.append({'NAME_NODE': node_name,'LABEL_NODE': node_name, 'FILTER': None, 'FUNCTION': 'Query', 'JOIN_ARG': None, 'COLOR': \"green\"})\n",
    "        nodes.append({'NAME_NODE': dest_table,'LABEL_NODE': dest_table, 'FILTER': None, 'FUNCTION': 'DataDestinations', 'JOIN_ARG': None, 'COLOR': \"green\"})\n",
    "\n",
    "\n",
    "        for column, transformation in zip(columns, transformation):\n",
    "            lineages.append({'SOURCE_COLUMNS':f'{node_name}.{column}', 'TARGET_COLUMN':f\"{dest_table}.{column}\", 'TRANSFORMATION':transformation})\n",
    "\n",
    "\n",
    "    return nodes, lineages, variable_tables\n",
    "\n",
    "\n",
    "def foreachloop_parser(control_flow, nodes, lineages, variable_tables, node_name):\n",
    "\n",
    "    nodes.append({'NAME_NODE': node_name,'LABEL_NODE': node_name, 'FILTER': None, 'FUNCTION': 'ForEachLoopContainer', 'JOIN_ARG': None, 'COLOR': \"gold\"})\n",
    "\n",
    "    executesql_parser(control_flow, nodes, lineages, variable_tables, node_name)\n",
    "\n",
    "    variables = control_flow[node_name]['Iterr_variables']\n",
    "    input_table = control_flow[node_name]['Input_variable']\n",
    "\n",
    "    for variable_table in variable_tables:\n",
    "        if input_table == variable_table: # if the variable table correspond to the input table\n",
    "            for column in variable_tables[variable_table]:\n",
    "                for variable in variables:\n",
    "                    if column[1] == variable[1]:\n",
    "                        lineages.append({'SOURCE_COLUMNS':f'{input_table}.{column}', 'TARGET_COLUMN':f\"{node_name}.{column}\", 'TRANSFORMATION':\"\"}) # CORRESPONDING COLUMN\n",
    "                        lineages.append({'SOURCE_COLUMNS':f'{node_name}.{variable}', 'TARGET_COLUMN':f\"{node_name}.{variable}\", 'TRANSFORMATION':\"\"})\n",
    "\n",
    "    return nodes, lineages\n",
    "\n",
    "\n",
    "def parse_sql_queries(control_flow:dict):\n",
    "\n",
    "    nodes = []\n",
    "    lineages = []\n",
    "    variable_tables = {} # dictionaries with temporary tables and their columns\n",
    "\n",
    "    for node in control_flow.keys():\n",
    "        if control_flow[node]['Description'] == 'Execute SQL Task':\n",
    "            nodes, lineages, variable_tables = executesql_parser(control_flow, nodes, lineages, variable_tables, node)\n",
    "\n",
    "        elif control_flow[node]['Description'] == 'Foreach Loop Container':\n",
    "            nodes, lineages = foreachloop_parser(control_flow, nodes, lineages, variable_tables, node)\n",
    "                    \n",
    "    nodes_df = pd.DataFrame(nodes)\n",
    "    nodes_df['ID'] = nodes_df.index\n",
    "    #nodes_df.to_csv(f'output-data/nodes/nodes-control_flow.csv',index=False)\n",
    "\n",
    "    lineages_df = pd.DataFrame(lineages)\n",
    "    lineages_df['SOURCE_FIELD'] = lineages_df['SOURCE_COLUMNS'].str.split('.', expand=True)[1]\n",
    "    lineages_df['TARGET_FIELD'] = lineages_df['TARGET_COLUMN'].str.split('.', expand=True)[1]\n",
    "    lineages_df['SOURCE_NODE'] = lineages_df['SOURCE_COLUMNS'].str.split('.', expand=True)[0]\n",
    "    lineages_df['TARGET_NODE'] = lineages_df['TARGET_COLUMN'].str.split('.', expand=True)[0]\n",
    "    lineages_df['LINK_VALUE'] = 1\n",
    "    lineages_df['ROW_ID'] = lineages_df.index\n",
    "    lineages_df['COLOR'] = 'aliceblue'\n",
    "    # merge source id\n",
    "    lineages_df = pd.merge(lineages_df, nodes_df[['ID', 'LABEL_NODE']], left_on='SOURCE_NODE', right_on = 'LABEL_NODE', how='left')\n",
    "    lineages_df['SOURCE_NODE'] = lineages_df['ID']\n",
    "    lineages_df.drop(columns=['ID', 'LABEL_NODE'], inplace=True)\n",
    "    # merge target id\n",
    "    lineages_df = pd.merge(lineages_df, nodes_df[['ID', 'LABEL_NODE']], left_on='TARGET_NODE', right_on = 'LABEL_NODE', how='left')\n",
    "    lineages_df['TARGET_NODE'] = lineages_df['ID']\n",
    "    lineages_df.drop(columns=['ID', 'LABEL_NODE'], inplace=True)\n",
    "    lineages_df = lineages_df.drop_duplicates(subset =['SOURCE_COLUMNS', 'TARGET_COLUMN', 'TRANSFORMATION']).reset_index(drop=True)\n",
    "    #lineages_df.to_csv(f'output-data/lineages/lineage-control_flow.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    with open('../../output-data/nodes/metadata_nodes_controlflow.json', 'r') as json_file: # columns data\n",
    "        control_flow = json.load(json_file)\n",
    "\n",
    "    parse_sql_queries(control_flow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Insert(\n",
       "  this=Schema(\n",
       "    this=Table(\n",
       "      this=Identifier(this=Order_write, quoted=False)),\n",
       "    expressions=[\n",
       "      Identifier(this=OrderID, quoted=False),\n",
       "      Identifier(this=Dateloop, quoted=False),\n",
       "      Identifier(this=Cat_ID, quoted=False)]),\n",
       "  expression=Values(\n",
       "    expressions=[\n",
       "      Tuple(\n",
       "        expressions=[\n",
       "          Literal(this=User::Sup_id, is_string=True),\n",
       "          CurrentTimestamp(),\n",
       "          Literal(this=User::Cat_id, is_string=True)])]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_query(\"INSERT INTO SSIS_Logger (ExecutionStartTime, Remark, Affected_rows) VALUES (GETDATE(), 'Deleted rows', 0)\")\n",
    "\n",
    "\n",
    "parse_query(\"INSERT INTO Order_write (OrderID, Dateloop, Cat_ID) VALUES ('User::Sup_id', GETDATE(), 'User::Cat_id')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rabo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
